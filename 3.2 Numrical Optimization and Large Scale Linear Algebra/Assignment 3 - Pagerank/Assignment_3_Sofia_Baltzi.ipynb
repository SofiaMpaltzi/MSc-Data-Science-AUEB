{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pagerank Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "from numpy.linalg import norm\n",
    "import time\n",
    "#np.seterr(divide='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    # read data\n",
    "    data = pd.read_csv(filename, sep='\\t', header=None, names=['from', 'to','weight'])\n",
    "    # reduce from and to values by 1 so that nodes start from 0 (for easier calculations)\n",
    "    data['from'] = data['from']-1\n",
    "    data['to'] = data['to']-1\n",
    "    \n",
    "    # number of edges\n",
    "    edges = len(data)    \n",
    "    \n",
    "    # dict{webpage:number of times webpage was a \"from\" page}\n",
    "    out_dict = data.groupby('from')['to'].count().to_dict()\n",
    "    # dict{webpage: list of webpages that lead to this webpage}\n",
    "    in_dict = data.groupby('to')['from'].aggregate(lambda x: list(x)).to_dict()\n",
    "    \n",
    "\n",
    "    # number of nodes\n",
    "    nodes = len(set(out_dict) | set(in_dict))\n",
    "              \n",
    "    # update in_dict with unseen nodes\n",
    "    for node in range(nodes):  \n",
    "        if node not in in_dict.keys():\n",
    "             in_dict[node] = []  \n",
    "    \n",
    "    # connectivity sparse matrix\n",
    "    sparse_m = csr_matrix(([1]*edges,(data['to'].tolist(), data['from'].tolist())), shape=(nodes, nodes))\n",
    "                \n",
    "    return in_dict, out_dict, nodes, edges, sparse_m\n",
    "\n",
    "\n",
    "#     D = sparse.lil_matrix((nodes, 1))\n",
    "#     for key in dict_out.keys():\n",
    "#         D[key] = 1/dict_out[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind, outd, nodes, edges, sp_m = read_data('stanweb.dat\\stanweb.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "def pagerank_power(G, s = .85, maxerr = .0001):\n",
    "    \"\"\"\n",
    "    Computes the pagerank for each of the n states\n",
    "    Parameters\n",
    "    ----------\n",
    "    G: matrix representing state transitions\n",
    "       Gij is a binary value representing a transition from state i to j.\n",
    "    s: probability of following a transition. 1-s probability of teleporting\n",
    "       to another state.\n",
    "    maxerr: if the sum of pageranks between iterations is bellow this we will\n",
    "            have converged.\n",
    "    \"\"\"\n",
    "    n = G.shape[0]\n",
    "\n",
    "    # transform G into markov matrix A\n",
    "    A = csc_matrix(G,dtype=np.float)\n",
    "    rsums = np.array(A.sum(1))[:,0]\n",
    "    ri, ci = A.nonzero()\n",
    "    A.data /= rsums[ri]\n",
    "\n",
    "    # bool array of sink states\n",
    "    sink = rsums==0\n",
    "\n",
    "    # Compute pagerank r until we converge\n",
    "    ro, r = np.zeros(n), np.ones(n)\n",
    "    while np.sum(np.abs(r-ro)) > maxerr:\n",
    "        ro = r.copy()\n",
    "        # calculate each pagerank at a time\n",
    "        for i in xrange(0,n):\n",
    "            # inlinks of state i\n",
    "            Ai = np.array(A[:,i].todense())[:,0]\n",
    "            # account for sink states\n",
    "            Di = sink / float(n)\n",
    "            # account for teleportation to state i\n",
    "            Ei = np.ones(n) / float(n)\n",
    "\n",
    "            r[i] = ro.dot( Ai*s + Di*s + Ei*(1-s) )\n",
    "\n",
    "    # return normalized pagerank\n",
    "    return r/float(sum(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank_galgo(G, alpha=0.85, tol=10**-8):\n",
    "\n",
    "    n = G.shape[0]\n",
    "\n",
    "    out_alpha = G.sum(axis=0).T/alpha\n",
    "    r = np.ones((n,1))/n\n",
    "    error = 1\n",
    "    t = 0\n",
    "    while error > tol:\n",
    "        t += 1\n",
    "        r_new = G.dot((r/out_alpha))\n",
    "        #  Lâ‰¤alpha due to dead-ends\n",
    "        L = r_new.sum()\n",
    "        # re-insert leaked page-rank\n",
    "        # now all ranks add to one\n",
    "        r_new += (1-L)/n\n",
    "        # compute the error as the euclidean norm of the\n",
    "        # previous ranks and the new ranks\n",
    "        error = np.linalg.norm(r-r_new)\n",
    "        # store the new ranks as the ranks of the current\n",
    "        # iteration\n",
    "        if t == 2:\n",
    "            # list to return the nodes that converged from the\n",
    "            # second iteration\n",
    "            list_conv = []\n",
    "            for i in range(r.shape[0]):\n",
    "                if np.linalg.norm(r[i]-r_new[i]) < tol:\n",
    "                    list_conv.append(i)\n",
    "        r = r_new\n",
    "    return r, t, list_conv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
